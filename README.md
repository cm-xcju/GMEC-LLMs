# GMEC-LLMs
[Enhanced Generative Framework with LLMs for Multimodal Emotion-Cause Pair Extraction in Conversations](https://ieeexplore.ieee.org/abstract/document/10891643)

![image](https://github.com/user-attachments/assets/87df521d-2438-45df-8ff7-9d2425a7ecc0)

## Tables of Contents
- [Environment](#Environment)
- [Visual Caption](#VisualCaption)
- [Dataset](#Dataset)
- [Usage](#usage)
- [Evaluation](#evaluation)
- [Citation](#citation)
- [License](#license)

## Environment  <a name="Environment"></a>

* [Python](https://www.python.org/downloads/) >= 3.9
* [Cuda](https://developer.nvidia.com/cuda-toolkit) >= 11.3
* [Pytorch](https://pytorch.org/get-started/locally/) >= 12.0
* The details can be find in [environment.yml](environment.yml)


## Visual Caption <a name="VisualCaption"></a>
We use the [MinGPT-4](https://github.com/ai-liam/NLP-MiniGPT-4) to extact the visual caption.
1. download the MinGPT-4 and its 13B checkpoint.
2. Add the image_demo files into the folder. We revise the [image_demo.py](MinGPT-4/image_demo.py) to adapt our task
3. Extract the Caption about the Image, and save to json file.

## Dataset <a name="Dataset"></a>

## Usage <a name="usage"></a>
### step 1:

##
The code will be open later. If you have any question, you can email xcju@stu.suda.edu.cn
